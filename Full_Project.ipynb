{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baseline DL Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JamesDeAntonis/Music_Stem_Separation/blob/master/Full_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xujpWpl9MTK-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################  FINAL PROJECT JAMIE DEANTONIS & RYAN MCNALLY #####################\n",
        "###### DATASET HOSTED IN GOOGLE DRIVE @ THIS LINK:\n",
        "###### https://drive.google.com/open?id=1iDNHD1Q0QrQ8EV0MnISyNHvx3bVOeBzs\n",
        "\n",
        "###### THIS NOTEBOOK WILL RUN END TO END\n",
        "###### SUMMARY: CLASSES AND METHODS FOR DATA PREPROCESSING AND MODEL BUILDING COME FIRST, SIMPLE EXECUTIONS AT THE BOTTOM\n",
        "###### FINAL OUTPUT FILE WILL BE WRITTEN FOR YOU AND FILEPATH IS SPECIFIED IN THE FINAL CELL, FEEL FREE TO CHANGE THE FILENAME TO SAVE A NEW FILE TO TEST"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzgs8rNFNSv_",
        "colab_type": "code",
        "outputId": "57d977b0-efbc-4e61-b01a-d03083a88f57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "# !git clone \n",
        "# https://gist.github.com/josephernest/3f22c5ed5dabf1815f16efa8fa53d476wav\n",
        "# !pip install sox\n",
        "# !sox old.wav -b 16 new.wav\n",
        "# import josephernest's remake of scipy.io.wavfile with readable 24-bit\n",
        "# https://gist.github.com/josephernest/3f22c5ed5dabf1815f16efa8fa53d476\n",
        "# from wav import wavfile as wav_reader\n",
        "!pip install pysoundfile\n",
        "!pip install simpleaudio\n",
        "!pip install playsound"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pysoundfile\n",
            "  Downloading https://files.pythonhosted.org/packages/2a/b3/0b871e5fd31b9a8e54b4ee359384e705a1ca1e2870706d2f081dc7cc1693/PySoundFile-0.9.0.post1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cffi>=0.6 in /usr/local/lib/python3.6/dist-packages (from pysoundfile) (1.13.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=0.6->pysoundfile) (2.19)\n",
            "Installing collected packages: pysoundfile\n",
            "Successfully installed pysoundfile-0.9.0.post1\n",
            "Collecting simpleaudio\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/1b/4dc29653733202b68c09d9c6ca085cf67ac54859ee860647ef21ac1ff3dc/simpleaudio-1.0.4.tar.gz (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 4.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: simpleaudio\n",
            "  Building wheel for simpleaudio (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for simpleaudio: filename=simpleaudio-1.0.4-cp36-cp36m-linux_x86_64.whl size=2063918 sha256=a0a74c30e7f015a0b48e3ff4e68e4c4d456407c83e1bdb60bb3db264496d233f\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/16/f5/74445bada9e7defeb1d1198d00f5f17f3519f633216b5c9267\n",
            "Successfully built simpleaudio\n",
            "Installing collected packages: simpleaudio\n",
            "Successfully installed simpleaudio-1.0.4\n",
            "Collecting playsound\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/16/10d897b0a83fb4b05b03a63d7a2667ab75f857f67f7062fd447dd3f49bf7/playsound-1.2.2-py2.py3-none-any.whl\n",
            "Installing collected packages: playsound\n",
            "Successfully installed playsound-1.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s28baTgxmFTM",
        "colab_type": "code",
        "outputId": "ea03c461-bfeb-4e2e-c954-75c41f5c4198",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        }
      },
      "source": [
        "from pathlib import Path  # for file handling\n",
        "from google.colab import drive  # for mounting the google drive\n",
        "import os  # for file handling\n",
        "import scipy.io.wavfile as wf  # for reading wav files NO WORK FOR 24BIT\n",
        "import soundfile  # for converting 24 bit to 16 bit\n",
        "from scipy import signal  # for the spectrogram\n",
        "from scipy import misc\n",
        "import matplotlib.pyplot as plt  # for plotting spectrograms\n",
        "import numpy as np\n",
        "import simpleaudio as sa  # good for playing the audio directly\n",
        "from playsound import playsound  # also good for playing audio\n",
        "import tensorflow as tf\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D\n",
        "from keras.layers import MaxPooling2D, Concatenate\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras import Input, Model\n",
        "import scipy.misc  # for writing the fourier transform to file\n",
        "import tensorflow\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from keras.layers import Concatenate\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras import Input, Model, Sequential\n",
        "import shutil  # for removing folders\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v03SKd0glpFv",
        "colab_type": "code",
        "outputId": "2c983f3b-db02-45e4-85e9-ce776e2c80a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "drive.mount('/content/drive')  # mount the google drive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ux1LmR7lDiv",
        "colab_type": "code",
        "outputId": "4abab6cf-8894-4473-dc6a-f1878e08077b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "os.chdir('/content/drive/My Drive/Stems')  # directory of the stems\n",
        "data_dir = Path('./Raw')  # formalize as a path-like object\n",
        "print(data_dir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Raw\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOekc0yrAaBa",
        "colab_type": "text"
      },
      "source": [
        "song_obj\n",
        "\n",
        "  attributes:\n",
        "\n",
        "    stem_list: list of stem objects that fit into that song\n",
        "    audio: an audio object, the audio data for this song\n",
        "    fourier (optional): a fourier object, the fourier transform for the song\n",
        "  functions:\n",
        "\n",
        "    compile_song: gets the audio data\n",
        "\n",
        "stem_obj\n",
        "\n",
        "  attributes:\n",
        "\n",
        "    filepath: the path to the file containing the stem?\n",
        "    song: the name of the song\n",
        "    stem: the name of the stem\n",
        "    audio: audio object, the audio data for this stem\n",
        "    fourier (optional): a fourier object, the fourier tranform for the stem\n",
        "  \n",
        "  functions:\n",
        "\n",
        "    none\n",
        "  \n",
        "audio_obj\n",
        "\n",
        "  attributes:\n",
        "\n",
        "    data: the numpy array of data for the audio\n",
        "    sr: the samplerate of the audio data\n",
        "  \n",
        "  functions:\n",
        "\n",
        "    write_audio_to_file: writes audio to file\n",
        "  \n",
        "fourier_obj\n",
        "\n",
        "  attributes:\n",
        "\n",
        "    freqs\n",
        "    times\n",
        "    fourier\n",
        "    sr\n",
        "    audio\n",
        "\n",
        "  functions:\n",
        "\n",
        "    plot: plots the log of the fourier (log is for visual clarity)\n",
        "    fourier_to_audio: converts the fourier and sr to an audio object, initializes the audio attribute"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSltakPltH5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class song_obj:  # SONG OBJECT (collection of stems)\n",
        "  def __init__(self, stem_list, name=None, get_fourier=False):\n",
        "    self.stem_list = stem_list\n",
        "    if name != None:\n",
        "      self.name = name\n",
        "    else:\n",
        "      self.name= stem_list[0].song_name\n",
        "    self.audio = self.compile_song()  # returns an audio object\n",
        "    if(get_fourier==True):\n",
        "      self.fourier = fourier_obj(self)\n",
        "    else:\n",
        "      self.fourier = None\n",
        "  \n",
        "  def compile_song(self):\n",
        "    sr_list = [stem.audio.sr for stem in self.stem_list]\n",
        "    assert(all(x == sr_list[0] for x in sr_list))\n",
        "    # dim1 = 15000000  # just to ensure all files have same shape\n",
        "    data_list = [stem.audio.data for stem in self.stem_list]\n",
        "    # assert(max([data.shape[0] for data in data_list]) < dim1)\n",
        "    # dim2 = max([data.shape[1] for data in data_list])\n",
        "    # audio = np.zeros((dim1))\n",
        "    length = max([data.shape[0] for data in data_list])\n",
        "    audio = np.zeros(length)\n",
        "    for data in data_list:\n",
        "      audio[:data.shape[0]] += data  # , :data.shape[1]] += data\n",
        "    return audio_obj(data=audio, sr=sr_list[0])\n",
        "  \n",
        "  def get_stem_splits(self):\n",
        "    vocals_stem_list = []\n",
        "    perc_stem_list = []\n",
        "    bass_stem_list = []\n",
        "    other_stem_list = []\n",
        "    for stem in self.stem_list:\n",
        "      stem_group = stem_dict[stem.stem_name]\n",
        "      if stem_group == 'Vocals':\n",
        "        vocals_stem_list.append(stem)\n",
        "      elif stem_group == 'Percussion':\n",
        "        perc_stem_list.append(stem)\n",
        "      elif stem_group == 'Bass':\n",
        "        bass_stem_list.append(stem)\n",
        "      elif stem_group == 'Other':\n",
        "        other_stem_list.append(stem)\n",
        "      else:\n",
        "        print('warning: ', stem.stem_name, 'was not added to any group')\n",
        "    return [song_obj(vocals_stem_list, name='Vocals'), \n",
        "            song_obj(perc_stem_list, name='Percussion'),\n",
        "            song_obj(bass_stem_list, name='Bass'),\n",
        "            song_obj(other_stem_list, name='Other')]\n",
        "\n",
        "\n",
        "class stem_obj:  # STEM OBJECT\n",
        "  def __init__(self, song_name, stem_name, get_fourier=False):\n",
        "    self.filepath = os.path.join('Raw', song_name, stem_name)\n",
        "    self.song_name = song_name\n",
        "    self.stem_name = stem_name\n",
        "    self.audio = audio_obj(filepath=self.filepath)\n",
        "    if(get_fourier==True):\n",
        "      self.fourier = fourier_obj(self.audio)\n",
        "    else:\n",
        "      self.fourier = None\n",
        "\n",
        "class audio_obj:  # AUDIO OBJECT\n",
        "  def __init__(self, data=None, sr=None, filepath=-1):\n",
        "    # one of (audio, sr) or (filepath) is required\n",
        "    # this is often the cause of problems if wrong if statement entered\n",
        "    if filepath != -1:\n",
        "      self.data, self.sr = soundfile.read(filepath)\n",
        "      if len(self.data.shape) >= 2:  # stereo\n",
        "        self.data = self.data[:, 0]  # need to think about stereo handling\n",
        "        # self.data = np.mean(data, axis=1)  # i think this is correct\n",
        "    else:\n",
        "      assert isinstance(data, np.ndarray), \"specify filepath or data\"\n",
        "      self.data = data\n",
        "      self.sr = sr\n",
        "      if isinstance(data, tuple):\n",
        "        self.data = self.data[0]\n",
        "    assert self.data.ndim == 1, \"audio data must be 1d\"\n",
        "\n",
        "  def to_fourier(self):  # convert to fourier\n",
        "    self.fourier = fourier_obj(self)\n",
        "    return self.fourier\n",
        "  \n",
        "  def to_file(self, filepath):  # save as .wav\n",
        "    soundfile.write(filepath, self.data, self.sr)\n",
        "  \n",
        "  def boxplot(self):  # boxplot of the spectrum of values in this audio\n",
        "    plt.boxplot(self.data)\n",
        "\n",
        "class fourier_obj:  # FOURIER OBJECT\n",
        "  def __init__(self, audio=-1,  # input is either this line\n",
        "               fourier=-1, freqs=-1, t=-1, sr=44100,  # or this line\n",
        "               abs=False, get_2d=True, get_ri=True):\n",
        "    # freqs : frequencies : freqs corresponding to 1st axis of fourier\n",
        "    # t     : times       : times corresponding to 2nd axis of fourier\n",
        "    # fourier : fourier series : 2d np array of fourier\n",
        "    assertion = not isinstance(audio, int) or not isinstance(fourier, int)\n",
        "    assert assertion, \"must either specify audio_obj or fourier array\"\n",
        "    if isinstance(fourier, int):\n",
        "      self.m = 1024\n",
        "      [self.freqs, self.t, fourier] = signal.stft(audio.data,\n",
        "                                                       nperseg=self.m,\n",
        "                                                       noverlap=3*self.m//4)\n",
        "      self.sr = sr\n",
        "      self.fourier_ri = self.get_two_channel_fourier(fourier)\n",
        "    else:  # if just passing in a fourier to make the object\n",
        "      if fourier.ndim == 2:\n",
        "        fourier = fourier\n",
        "        self.m = fourier.shape[0] * 2 - 2\n",
        "        self.freqs = freqs\n",
        "        self.t = t\n",
        "        self.sr = sr\n",
        "        self.fourier_ri = self.get_two_channel_fourier(fourier)\n",
        "      else:  # the fourier defined is actually the fourier_ri\n",
        "        assert fourier.ndim == 3, \"fourier_obj must have ndim 2 or 3\"\n",
        "        self.fourier_ri = fourier\n",
        "        # self.fourier = self.ri_to_complex()\n",
        "        self.m = self.fourier_ri.shape[0] * 2 - 2\n",
        "        self.freqs = freqs\n",
        "        self.t = t\n",
        "        self.sr = sr\n",
        "    if abs == True:\n",
        "      self.fourier = np.abs(self.fourier)\n",
        "    assert self.fourier_ri.ndim == 3, \"fourier ri must be 3d\"\n",
        "  \n",
        "  def get_two_channel_fourier(self, fourier):\n",
        "    # for real in first channel, imaginary in second\n",
        "    f = np.zeros(fourier.shape + tuple((2,)))\n",
        "    f[:, :, 0] = np.real(fourier)\n",
        "    f[:, :, 1] = np.imag(fourier)\n",
        "    return f\n",
        "  \n",
        "  def ri_to_complex(self):\n",
        "    # convert the 2-channel fourier to a standard 2d fourier\n",
        "    real = np.array(self.fourier_ri[:, :, 0], dtype=complex)\n",
        "    imag = np.array(self.fourier_ri[:, :, 1], dtype=complex) * 1j\n",
        "    return real + imag\n",
        "  \n",
        "  def to_audio(self):  # convert to audio\n",
        "    fourier = self.ri_to_complex()\n",
        "    ts, data = signal.istft(fourier, self.sr,\n",
        "                            nperseg=self.m, noverlap=3*self.m//4)\n",
        "    self.audio = audio_obj(data=data, sr=self.sr)\n",
        "    return self.audio\n",
        "  \n",
        "  def to_file(self, songname, filename):  # save as jpg\n",
        "    path = os.path.join('Train', songname, 'TRANSFORM', filename + '.jpg')\n",
        "    scipy.misc.imsave(path, self.fourier)\n",
        "  \n",
        "  def plot(self):\n",
        "    # log makes the plot more readable\n",
        "    fourier_like = np.log(np.abs(self.fourier).clip(min=0.0000000001))\n",
        "    plt.pcolormesh(self.t, self.freqs, fourier_like)\n",
        "    plt.ylabel('Frequency [Hz]')\n",
        "    plt.xlabel('Time [sec]')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4pSZlL_4I-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stem_dict_init = {\n",
        "    'Vocals'     : ['BG Vox', 'Lead Vox', 'Vox Chop', 'Vocals', \n",
        "                    'Ahh Stem', 'Drew Ad Lib Stem', 'Drew LD Vox Stem',\n",
        "                    'Emily Vox Stem', 'Gang Vocal Stem', 'Hook Stem', \n",
        "                    'Vocoder Stem', 'Vox Chop Stem', 'Vox Pad Stem',\n",
        "                    'Vocal Filter', 'Vocal Glitch'],\n",
        "    'Percussion' : ['Drums', 'Claps', 'Hi Hats', 'Snares', 'Crash Stem',\n",
        "                    'Hi Hats Stem', 'Kick Stem', 'SFX Stem', 'Snare Stem',\n",
        "                    'Toms Stem', 'Clap', 'Hi Hats & Cymbals', 'Impacts',\n",
        "                    'Kick', 'Live Drums', 'Percussion', 'Toms'],\n",
        "    'Bass'       : ['Bass', '808', 'Bass Stem', 'Wide Bass Stem', \n",
        "                    'Live Bass'],\n",
        "    'Other'      : ['Glitch Synth', 'GTR', 'Piano', 'Sax', 'SFX',\n",
        "                    'Strings & Slide GTR', 'Synth Chords',\n",
        "                    'Guitars', 'Risers', 'Synths', 'Arp Stem',\n",
        "                    'BGV Stem', 'GTR Stem', 'Horns Stem', 'Keys Stem',\n",
        "                    'Pad Stem', 'Piano Stem', 'Pluck Stem', \n",
        "                    'Sax Pad Stem', 'Sax Stem', 'Strings Stem', \n",
        "                    'Acoustic Guitar', 'Arp Synth', 'Drop Layers',\n",
        "                    'Drop Synth 2', 'E Guitar 2', 'Fills', 'Final Synth',\n",
        "                    'Piano 1', 'Piano 2', 'Wurli']\n",
        "}\n",
        "\n",
        "stem_dict = {}  # \n",
        "for group in stem_dict_init.keys():\n",
        "  for stem in stem_dict_init[group]:\n",
        "    stem_dict[stem + '.wav'] = group"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-cAIwpIpwXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compile_tracks_and_stems():\n",
        "  # loops through each song, makes (1) the full track file\n",
        "  #                                (2) the vocal, perc, bass, other tracks\n",
        "  for song in os.listdir(data_dir):  # loop through each song\n",
        "    print('*****' + song)\n",
        "    stems = []  # intialize what will be a nested list of stem objects\n",
        "\n",
        "    # (1) make the full track file\n",
        "    for stem in os.listdir(os.path.join(data_dir, song)):  # stems in song\n",
        "      # print('*****   ', song, '*****  ', stem)\n",
        "      temp = stem_obj(song, stem, get_fourier=False)\n",
        "      stems.append(temp)  # add current stem to the list\n",
        "    song = song_obj(stem_list=stems)\n",
        "    song.audio.write_to_file(os.path.join('Train',  # full song file\n",
        "                                          song.name,\n",
        "                                          'FULL_TRACK/song.wav'))\n",
        "    \n",
        "    # (2) make the vocal, perc, bass, other tracks\n",
        "    for stem_split in song.get_stem_splits():\n",
        "      # write the full stems into files\n",
        "      stem_split.audio.write_to_file(os.path.join('Train',  # stemgroup file\n",
        "                                                  song.name,\n",
        "                                                  'STEMS', \n",
        "                                                  stem_split.name + '.wav'))\n",
        "\n",
        "# compile_tracks_and_stems()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1C9IjljYDJBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class koretzky_obj:\n",
        "  def __init__(self, stem_name, n, is_regression=False):\n",
        "    self.me = model_essentials(n=n, is_regression=is_regression)\n",
        "    self.stem_name = stem_name\n",
        "    first = True\n",
        "    mew = True\n",
        "    for i, song in enumerate(os.listdir(data_dir)):\n",
        "      if song == 'DLMD':\n",
        "        continue\n",
        "      if first == True:  # first iteration only\n",
        "        track = koretzky_track_obj(song, stem_name, self.me)\n",
        "        inputs = track.full_track.inputs\n",
        "        targets = track.stem.outputs\n",
        "        first = False\n",
        "        del track\n",
        "        break\n",
        "        continue\n",
        "      # second iteration and beyond:\n",
        "      inputs = np.concatenate((inputs, \n",
        "                               self.get_inputs(song_name=song, mew=mew)))\n",
        "      targets = np.concatenate((targets, \n",
        "                                self.get_targets(song_name=song,\n",
        "                                                 stem_name='Percussion',\n",
        "                                                 is_reg=self.me.is_regression,\n",
        "                                                 mew=mew)))\n",
        "      if i >= 2:\n",
        "        break\n",
        "    self.inputs = inputs\n",
        "    self.targets = targets\n",
        "    self.model = self.koretzky()\n",
        "    \n",
        "  def koretzky(self):\n",
        "    shape = (self.me.m, self.me.n, 2)\n",
        "    input_shape = Input(shape=shape, name='input')\n",
        "    # # concatenated_stems = []\n",
        "    for i in range(1):\n",
        "      i = str(i + 1)\n",
        "      x = Conv2D(32, (3,3), padding='same',input_shape=shape)(input_shape)\n",
        "      x = LeakyReLU(name='leaky_' + i + '1')(x)\n",
        "      x = Conv2D(16, (3, 3), padding='same', name='conv16_' + i + '1')(x)\n",
        "      x = LeakyReLU(name='leaky_' + i + '2')(x)\n",
        "      x = MaxPooling2D(pool_size=(3, 3), name='max33_' + i + '1')(x)\n",
        "      x = Dropout(0.25, name='dropout25_' + i + '1')(x)\n",
        "      x = Conv2D(64, (3, 3), padding='same', name='conv64_' + i)(x)\n",
        "      x = LeakyReLU(name='leaky_' + i + '3')(x)\n",
        "      x = Conv2D(16, (3, 3), padding='same', name='conv16_' + i + '2')(x)\n",
        "      x = LeakyReLU(name='leaky_' + i + '4')(x)\n",
        "      x = MaxPooling2D(pool_size=(3,3), name='max33_' + i + '2')(x)\n",
        "      x = Dropout(0.25, name='dropout25_' + i + '2')(x)\n",
        "      x = Flatten(name='flatten_' + i)(x)\n",
        "      x = Dense(128, name='dense128_' + i)(x)\n",
        "      x = LeakyReLU(name='leaky_' + i + '5')(x)\n",
        "      x = Dropout(0.5, name='dropout50_' + i)(x)\n",
        "      x = Dense(self.me.m * 2, activation=self.me.activation,\n",
        "                name='dense513_' + i)(x)\n",
        "      # # concatenated_stems.append(x)\n",
        "    # concatenate output layers\n",
        "    # # merged_output=Concatenate(axis=1,name='concat')(concatenated_stems)\n",
        "    # connect to single/common input layer\n",
        "    # # model = Model(input_shape, merged_output)\n",
        "    model = Model(input_shape, x)\n",
        "    model.compile(optimizer=self.me.opt,\n",
        "                  loss=self.me.loss,\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "  \n",
        "  def predict_track(self, song):  # koretzky_track_obj\n",
        "    #  input_obj = koretzky_input_obj(song, self.me)\n",
        "    input_slices = self.get_inputs(song)  # input_obj.slices.slices  # a numpy array\n",
        "    print('got input slices')\n",
        "    fourier = fourier_slices_obj(input_slices).to_fourier(44100).fourier_ri  # input_obj.fourier_obj.fourier_ri\n",
        "    sr = 44100  # input_obj.sr\n",
        "    # del input_obj  # delete the object entirely\n",
        "    print('got fourier_slices')\n",
        "    preds = self.model.predict(input_slices)\n",
        "    print('got preds')\n",
        "    return koretzky_prediction_obj(track=fourier, sr=sr,\n",
        "                                   model_input=input_slices, \n",
        "                                   model_output=preds,\n",
        "                                   song=song,\n",
        "                                   stem=self.stem_name,\n",
        "                                   me=self.me)\n",
        "    \n",
        "  def output_slices_to_audio(self):\n",
        "    x = np.repeat(np.expand_dims(self.outputs[0][0], axis=2), \n",
        "                  self.n, axis=2)\n",
        "    y = np.reshape(x, (x.shape[0], x.shape[1] * x.shape[2]))\n",
        "    print(y.shape)\n",
        "    audio = fourier_obj(fourier=y, sr=self.srs[0]).to_audio()\n",
        "    audio.write_to_file(os.path.join('test_files', 'sliced_up2.wav'))\n",
        "    print(audio.data.shape)\n",
        "    print('file saved in test_files/sliced_up2.wav')\n",
        "    return(audio)\n",
        "  \n",
        "  def get_inputs(self, song_name, mew=False):\n",
        "    # turn the fourier into slices (either with overlapping window\n",
        "    # or non-overlapping window (mutually_exclusive_windows))\n",
        "    path = os.path.join('Train', song_name, 'FULL_TRACK', 'song.wav')\n",
        "    f = fourier_obj(audio_obj(filepath=path)).fourier_ri\n",
        "    n = self.me.n\n",
        "    shape = f.shape\n",
        "    shape1 = shape[1] // n\n",
        "    fs = f[:, :(n * shape1), :]  # clip for even division\n",
        "    # fs = np.real(fs)  # HOW TO HANDLE COMPLEX VALUES????\n",
        "    if mew == False:\n",
        "      temp_1 = np.zeros((fs.shape[0], fs.shape[1] + n - 1, 2))  # padding\n",
        "      temp_1[:, (n-1)//2 : temp_1.shape[1] - (n-1)//2, :] += fs\n",
        "      temp_2 = np.zeros((fs.shape[1], fs.shape[0], n, 2))\n",
        "      for i in range(temp_2.shape[0]):\n",
        "        temp_2[i, :, :, :] += temp_1[:, i : i+n, :]  # (i, freq, t, 2)\n",
        "    else:  # mutually_exclusive_windows == True\n",
        "      temp_1 = np.transpose(fs, (1, 0, 2))\n",
        "      temp_2 = np.reshape(temp_1, (temp_1.shape[0] // n,\n",
        "                                   n, temp_1.shape[1], 2))\n",
        "    assert temp_2.ndim == 4, \"temp_2 must be 4d\"\n",
        "    return temp_2\n",
        "  \n",
        "  def get_targets(self, song_name, stem_name, is_reg=False, mew=False):\n",
        "    path = os.path.join('Train', song_name, 'STEMS', stem_name + '.wav')\n",
        "    f = fourier_obj(audio_obj(filepath=path)).fourier_ri\n",
        "    if is_reg == False:  # if using a mask\n",
        "      mask_thresh = np.percentile(f.flatten(), 97)  # 97th pct threshold\n",
        "      f = np.where(f > mask_thresh, 1, 0)\n",
        "    n = self.me.n\n",
        "    shape = f.shape\n",
        "    shape1 = shape[1] // n\n",
        "    fs = f[:, :(n * shape1), :]  # clips for even division\n",
        "    fs = np.transpose(fs, (1, 0, 2))  # first axis should be timesteps\n",
        "    if mew == True:\n",
        "      fs = np.reshape(fs, (n, shape1, fs.shape[1], 2))  # (n, shp1, m, 2)\n",
        "      fs = fs[(n - 1) // 2, :, :, :]  # (shape1, m) (dim1 vanishes?)\n",
        "    assert fs.ndim == 3, \"fs must be 3d\"\n",
        "    shape = fs.shape\n",
        "    return np.reshape(fs, (shape[0], shape[1] * shape[2]))\n",
        "\n",
        "class model_essentials:  # model essential values\n",
        "  def __init__(self, is_regression=-1, n=-1):\n",
        "    self.is_regression = is_regression\n",
        "    self.n = n  # the number of timesteps in each slice\n",
        "    self.m = 513  # the first axis of fourier\n",
        "    if is_regression == True:\n",
        "      self.loss = 'mse'\n",
        "      self.opt = 'adam'\n",
        "      self.activation = 'linear'\n",
        "    else:\n",
        "      self.loss = 'binary_crossentropy'\n",
        "      self.opt = 'adam'\n",
        "      self.activation = 'sigmoid'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdq3jOkCMvlF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############ TRACK OBJECTS\n",
        "\n",
        "class koretzky_track_obj:\n",
        "  def __init__(self, song_name, stem, me):\n",
        "    assert me.n % 2 == 1, \"n must be an odd integer\"\n",
        "    self.full_track = koretzky_input_obj(song_name, me, mew=False)\n",
        "    # self.vocals = koretzky_target_obj(song_name, 'Vocals', \n",
        "    #                                   me, self.full_track)\n",
        "    # self.bass = koretzky_target_obj(song_name, 'Bass', \n",
        "    #                                 me, self.full_track)\n",
        "    self.stem = koretzky_target_obj(song_name, stem, \n",
        "                                    me, mew=False)\n",
        "    # self.other = koretzky_target_obj(song_name, 'Other', \n",
        "    #                                  me, self.full_track)\n",
        "    self.sr = self.full_track.sr\n",
        "\n",
        "class koretzky_input_obj:\n",
        "  def __init__(self, song_name, me, mew=False):\n",
        "    filepath = self.get_path(song_name)\n",
        "    assert me.n % 2 == 1, \"n must be an odd integer\"\n",
        "    self.n = me.n  # the number of samples in each split (25 per koretzky)\n",
        "    self.track = audio_obj(filepath=filepath)  # start with an audio obj\n",
        "    self.sr = self.track.sr  # follows from the audio obj\n",
        "    self.fourier_obj = fourier_obj(audio=self.track)\n",
        "    self.slices = self.get_slices_obj(mew)  # take slices of the raw track\n",
        "    data = self.slices.slices\n",
        "    #r = np.real(data)\n",
        "    #c = np.imag(data)\n",
        "    self.inputs = data #tf.complex(r, c)\n",
        "  \n",
        "  def get_path(self, song_name):\n",
        "    return os.path.join('Train', song_name, 'FULL_TRACK', 'song.wav')\n",
        "  \n",
        "  def apply_mask(self, mask):\n",
        "    f = self.fourier_obj\n",
        "    masked = f.fourier_ri * mask\n",
        "    f1 = fourier_obj(fourier=masked, freqs=f.freqs, t=f.t, sr=f.sr)\n",
        "    return f1\n",
        "  \n",
        "  def get_slices_obj(self, mutually_exclusive_windows=False):\n",
        "    # turn the fourier into slices (either with overlapping window\n",
        "    # or non-overlapping window (mutuall_exclusive_windows))\n",
        "    f = self.fourier_obj\n",
        "    n = self.n\n",
        "    shape = f.fourier_ri.shape\n",
        "    shape1 = shape[1] // n\n",
        "    fs = f.fourier_ri[:, :(n * shape1), :]  # clip for even division\n",
        "    # fs = np.real(fs)  # HOW TO HANDLE COMPLEX VALUES????\n",
        "    if mutually_exclusive_windows == False:\n",
        "      temp_1 = np.zeros((fs.shape[0], fs.shape[1] + n - 1, 2))  # padding\n",
        "      temp_1[:, (n-1)//2 : temp_1.shape[1] - (n-1)//2, :] += fs\n",
        "      temp_2 = np.zeros((fs.shape[1], fs.shape[0], n, 2))\n",
        "      for i in range(temp_2.shape[0]):\n",
        "        temp_2[i, :, :, :] += temp_1[:, i : i+n, :]  # (i, freq, t, 2)\n",
        "    else:  # mutually_exclusive_windows == True\n",
        "      temp_1 = np.transpose(fs, (1, 0, 2))\n",
        "      temp_2 = np.reshape(temp_1, (temp_1.shape[0] // n,\n",
        "                                   n, temp_1.shape[1], 2))\n",
        "    assert temp_2.ndim == 4, \"temp_2 must be 4d\"\n",
        "    return fourier_slices_obj(temp_2)\n",
        "\n",
        "class koretzky_target_obj:\n",
        "  def __init__(self, song_name, stem, me, mew=False):\n",
        "    assert me.n % 2 == 1, \"n must be an odd integer\"\n",
        "    filepath = self.get_path(song_name, stem)\n",
        "    self.n = me.n\n",
        "    self.track = audio_obj(filepath=filepath)  # start with an audio obj\n",
        "    self.sr = self.track.sr  # follows from the audio obj\n",
        "    if me.is_regression:\n",
        "      self.fourier_obj = self.get_mask_from_division()\n",
        "    else:\n",
        "      self.fourier_obj = self.get_mask_from_pct(97)\n",
        "    self.slices = self.get_slices_obj(mew)\n",
        "    shape = self.slices.slices.shape\n",
        "    self.outputs = self.slices.slices.reshape((shape[0],\n",
        "                                               shape[1] * 2))\n",
        "  \n",
        "  def get_path(self, song_name, stem):\n",
        "    return os.path.join('Train', song_name, 'STEMS', stem + '.wav')\n",
        "\n",
        "  def get_mask_from_pct(self, pct):\n",
        "    f = fourier_obj(audio=self.track)\n",
        "    mask_thresh = np.percentile(f.fourier_ri.flatten(), pct)\n",
        "    mask = np.where(f.fourier_ri > mask_thresh, 1, 0)\n",
        "    return fourier_obj(fourier=mask, freqs=f.freqs, t=f.t, sr=f.sr)\n",
        "  \n",
        "  def get_mask_from_division(self):#, full_song):\n",
        "    #f = fourier_obj(audio=self.track)\n",
        "    #mask = f.fourier / np.where(full_song.fourier_obj.fourier == 0,\n",
        "    #                            1e-9, full_song.fourier_obj.fourier)\n",
        "    # return fourier_obj(fourier=mask, freqs=f.freqs, t=f.t, sr=f.sr)\n",
        "    return fourier_obj(audio=self.track)  # let's just predict the stem\n",
        "\n",
        "  def get_slices_obj(self, mutually_exclusive_windows=False):\n",
        "    f = self.fourier_obj\n",
        "    n = self.n\n",
        "    shape = f.fourier_ri.shape\n",
        "    shape1 = shape[1] // n\n",
        "    fs = f.fourier_ri[:, :(n * shape1), :]  # clips for even division\n",
        "    fs = np.transpose(fs, (1, 0, 2))  # first axis should be timesteps\n",
        "    if mutually_exclusive_windows == True:\n",
        "      fs = np.reshape(fs, (n, shape1, fs.shape[1], 2))  # (n, shp1, m, 2)\n",
        "      fs = fs[(n - 1) // 2, :, :, :]  # (shape1, m) (dim1 vanishes?)\n",
        "    assert fs.ndim == 3, \"fs must be 3d\"\n",
        "    return fourier_slices_obj(fs)\n",
        "\n",
        "class koretzky_prediction_obj:\n",
        "  def __init__(self, track, sr, model_input, model_output, \n",
        "               song, stem, me):\n",
        "    # self.model_input = model_input\n",
        "    self.song = song\n",
        "    self.stem_name = stem\n",
        "    self.model_output = model_output  # np array of mask pred\n",
        "    # self.track = track  # koretzky track object of actual track\n",
        "    target_ri = koretzky_target_obj(song_name=song, \n",
        "                                    stem=stem, \n",
        "                                    me=me).fourier_obj.fourier_ri\n",
        "    slices = fourier_slices_obj(self.model_output)  # slices obj of MO\n",
        "    fourier = slices.to_fourier(sr=sr)  # fourier obj of MO\n",
        "    self.sr = sr  # sr\n",
        "    self.vox_preds = fourier.fourier_ri  # fourier ri data of MO\n",
        "    if me.is_regression == False:\n",
        "      self.target_audio = self.apply_mask(target_ri, \n",
        "                                          track)\n",
        "      self.predicted_audio = self.apply_mask(self.vox_preds, track)\n",
        "    else:  # regression == True\n",
        "      temp = fourier_slices_obj(model_input)\n",
        "      #self.input_audio = temp.to_fourier(sr=sr).fourier_ri\n",
        "      #targets = fourier_slices_obj(self.target.outputs)\n",
        "      self.target_audio = target_ri#  .to_fourier(sr=sr).fourier_ri  # np array\n",
        "      self.predicted_audio = self.vox_preds\n",
        "  \n",
        "  def apply_mask(self, target, track):\n",
        "    vox = np.zeros(track.shape)\n",
        "    vox[:min(vox.shape[0], target.shape[0]), \n",
        "        :min(vox.shape[1], target.shape[1]), \n",
        "        :] = target[:min(vox.shape[0], target.shape[0]),\n",
        "                    :min(vox.shape[1], target.shape[1]), :]\n",
        "    return track * vox\n",
        "  \n",
        "  def to_folder(self, folder_path):\n",
        "    path = os.path.join('test_files', self.song, folder_path)\n",
        "    if os.path.exists(path):\n",
        "      print('path already exists - deleting the old one')\n",
        "      shutil.rmtree(path)\n",
        "    os.mkdir(path)\n",
        "    # write the input to audio\n",
        "    # audio = fourier_obj(fourier=self.input_audio,\n",
        "    #                     sr=self.sr).to_audio()\n",
        "    # audio.to_file(os.path.join(path, 'input.wav'))\n",
        "    # write the prediction to audio\n",
        "    audio = fourier_obj(fourier=self.predicted_audio, \n",
        "                        sr=self.sr).to_audio()\n",
        "    audio.to_file(os.path.join(path, self.stem_name + '.wav'))\n",
        "    del audio\n",
        "    # write the output to audio\n",
        "    audio = fourier_obj(fourier=self.target_audio,\n",
        "                        sr=self.sr).to_audio()\n",
        "    audio.to_file(filepath=os.path.join(path, 'actual.wav'))\n",
        "\n",
        "\n",
        "class fourier_slices_obj:\n",
        "  def __init__(self, slice_array):#, ts_array, freqs_array):\n",
        "    if slice_array.ndim == 2:  # need to unflatten the output\n",
        "      shape = slice_array.shape\n",
        "      slice_array = np.reshape(slice_array, (shape[0], \n",
        "                                             shape[1] // 2, 2))\n",
        "    fs = slice_array\n",
        "    m = slice_array.shape[0]\n",
        "    self.slices = slice_array\n",
        "    # self.slice_objs = [fourier_obj(fourier=fs[i, :]) for i in range(m)]\n",
        "  \n",
        "  def to_fourier(self, sr):  # convert back to fourier\n",
        "    slices = np.array(self.slices)\n",
        "    if len(slices.shape) != 3:  # (m, freq_range, len_slice, channels)\n",
        "      assert len(slices.shape) == 4, \"must have shape len 3 or 4\"\n",
        "      mid = (slices.shape[2] - 1) // 2\n",
        "      slices = slices[:, :, mid, :]  # should turn into (m, freq_range, 2)\n",
        "    # slices are singleton columns of fourier (m, freq_range)\n",
        "    fourier_ri = np.transpose(slices, (1, 0, 2))\n",
        "    print('creating fourier object')\n",
        "    return fourier_obj(fourier=fourier_ri, sr=sr)  # will recognize ri"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IG8OrSE-XnmL",
        "colab_type": "text"
      },
      "source": [
        "Space below to implement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2G2KiepQd9Sl",
        "colab_type": "code",
        "outputId": "baad4956-11d1-45c6-848c-e4e13560edb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "source": [
        "# loads the tracks, gets the fouriers, apply masks, make slices\n",
        "n = 25  # width of each slice\n",
        "k = koretzky_obj('Percussion', n, is_regression=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORKr7ikq9IzN",
        "colab_type": "code",
        "outputId": "26779a3a-d47f-4f26-d6db-c2ee68103f87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        }
      },
      "source": [
        "# use slices as target, fit the model\n",
        "# slightly worse now cause predicts for both channels separately\n",
        "k.model.fit(x=k.inputs, y=k.targets, epochs=5)\n",
        "del k.inputs\n",
        "del k.targets"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/5\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "42650/42650 [==============================] - 38s 897us/step - loss: 0.1127 - acc: 0.9676\n",
            "Epoch 2/5\n",
            "42650/42650 [==============================] - 23s 543us/step - loss: 0.0823 - acc: 0.9690\n",
            "Epoch 3/5\n",
            "42650/42650 [==============================] - 24s 554us/step - loss: 0.0784 - acc: 0.9689\n",
            "Epoch 4/5\n",
            "42650/42650 [==============================] - 24s 554us/step - loss: 0.0766 - acc: 0.9688\n",
            "Epoch 5/5\n",
            "42650/42650 [==============================] - 24s 553us/step - loss: 0.0757 - acc: 0.9688\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACSKtpjllzED",
        "colab_type": "code",
        "outputId": "e1c410fa-a906-4524-dc99-a575f2afb3b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# predict slices on a song\n",
        "pred_obj = k.predict_track('CLOSER')  # RAM spikes and then goes down"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "got input slices\n",
            "creating fourier object\n",
            "got fourier_slices\n",
            "got preds\n",
            "creating fourier object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LErd_VyJ5iY6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prediction is saved to /content/drive/My Drive/Stems/test_files/[song_name]/actual.wav\n",
        "pred_obj.to_folder('test_12_20_331_regression_vRM')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8r99XzYopBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}